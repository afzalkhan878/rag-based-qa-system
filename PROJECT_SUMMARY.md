<h1 align="center">ğŸ“Œ Project Summary & Requirement Justification</h1> <p align="center"> <b>RAG-Based Question Answering System</b><br> Demonstrating Applied AI using Retrieval-Augmented Generation </p> <hr> <h2 style="color:#2E86C1;">ğŸ¯ Project Objective</h2> <p> The objective of this project is to design and implement a <b>Retrieval-Augmented Generation (RAG)</b> system that allows users to upload documents and ask questions whose answers are strictly grounded in the uploaded content. </p> <p> The system is intentionally built without heavy RAG frameworks to maintain transparency, explainability, and academic clarity. </p> <hr> <h2 style="color:#27AE60;">âœ… Functional Requirements Fulfillment</h2> <h3>ğŸ“„ Document Acceptance (PDF & TXT)</h3> <ul> <li>Supports <b>PDF</b> documents using <code>pypdf</code></li> <li>Supports <b>TXT</b> files via UTF-8 decoding</li> <li>Meets the requirement of handling at least two formats</li> </ul> <h3>âœ‚ï¸ Chunking & Embedding</h3> <ul> <li>Documents are cleaned and split into overlapping chunks</li> <li>Chunks are embedded using <b>sentence-transformers</b></li> <li>Ensures semantic consistency and retrieval accuracy</li> </ul> <h3>ğŸ§  Vector Store (FAISS)</h3> <ul> <li>Embeddings stored locally using <b>FAISS</b></li> <li>Provides fast, in-memory similarity search</li> <li>No external cloud dependency required</li> </ul> <h3>ğŸ” Retrieval of Relevant Chunks</h3> <ul> <li>User queries are embedded and matched via cosine similarity</li> <li>Top-k relevant chunks are retrieved</li> <li>Prevents hallucinated answers</li> </ul> <h3>ğŸ¤– Answer Generation using LLM</h3> <ul> <li>Uses a HuggingFace Question-Answering Transformer</li> <li>Fallback mechanism ensures robustness</li> <li>Answers are strictly grounded in retrieved context</li> </ul> <p> For answer generation, the system uses a lightweight open-source Large Language Model (FLAN-T5) to generate coherent, context-aware, and human-readable answers from the retrieved document chunks. </p> <hr> <h2 style="color:#AF7AC5;">âš™ï¸ Technical Requirements Fulfillment</h2> <ul> <li><b>FastAPI</b> used for API development</li> <li><b>Pydantic</b> ensures request validation</li> <li><b>FAISS</b> enables similarity search</li> <li><b>BackgroundTasks</b> handle document ingestion</li> <li><b>Rate Limiter</b> protects API usage</li> </ul> <hr> <h2 style="color:#E74C3C;">ğŸ“˜ Mandatory Explanations</h2> <h3>Why this Chunk Size?</h3> <p> A chunk size of approximately <b>2048 characters</b> with overlap was chosen to balance context preservation and retrieval precision while staying within embedding model limits. </p> <h3>Retrieval Failure Case</h3> <p> A documented case showed that even semantically similar chunks may fail to satisfy user intent (e.g., definition vs. procedural query), highlighting real-world RAG limitations. </p> <h3>Metric Tracked</h3> <p> <b>Query latency</b> was tracked as it directly affects user experience and reflects system scalability. </p> <hr> <h2 style="color:#1ABC9C;">ğŸ“¦ Deliverables</h2> <ul> <li>âœ” Public GitHub Repository</li> <li>âœ” Architecture Diagram</li> <li>âœ” Well-documented README</li> <li>âœ” Modular and explainable codebase</li> </ul> <hr> <h2 style="color:#2980B9;">ğŸ Conclusion</h2> <p> This project fulfills all functional, technical, and documentation requirements for a RAG-based Question Answering system. It demonstrates strong understanding of chunking strategy, retrieval quality, API design, and performance awareness while maintaining academic transparency. </p> <hr> <p align="center"> <b>Author:</b> Md Afzal Khan<br> B.Tech CSE (Data Science), Amity University </p>
