"""
Answer Generator Module
Generates answers using retrieved context and a generative LLM
"""

from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)


class AnswerGenerator:
    """
    Generates answers to questions using retrieved context
    and a generative language model (FLAN-T5)
    """

    def __init__(self):
        self.model = None
        self._initialize_model()

    def _initialize_model(self):
        try:
            from transformers import pipeline

            logger.info("Initializing generative LLM (FLAN-T5)...")

            self.model = pipeline(
                "text2text-generation",
                model="google/flan-t5-base",
                device=-1  # CPU
            )

            logger.info("LLM initialized successfully")

        except Exception as e:
            logger.warning(f"LLM not available, using fallback: {e}")
            self.model = None

    def generate_answer(
        self, question: str, context_chunks: List[Dict]
    ) -> Tuple[str, float]:

        if not context_chunks:
            return "I couldn't find relevant information to answer your question.", 0.0

        try:
            context = self._prepare_context(context_chunks)

            if self.model is not None:
                return self._generate_with_model(question, context, context_chunks)

            return self._generate_fallback(question, context_chunks)

        except Exception as e:
            logger.error(f"Answer generation error: {e}")
            return self._generate_fallback(question, context_chunks)

    def _prepare_context(
        self, context_chunks: List[Dict], max_length: int = 1500
    ) -> str:

        parts = []
        total = 0

        for chunk in context_chunks:
            text = chunk["text"]
            if total + len(text) <= max_length:
                parts.append(text)
                total += len(text)
            else:
                remaining = max_length - total
                if remaining > 0:
                    parts.append(text[:remaining])
                break

        return " ".join(parts)

    def _generate_with_model(
        self, question: str, context: str, context_chunks: List[Dict]
    ) -> Tuple[str, float]:
        """
        Generate answer using a generative LLM (FLAN-T5)
        """

        prompt = (
            "You are an expert AI tutor. Give a detailed, clear explanation.\n\n"
            "Use the context below to answer.\n\n"
            f"Context:\n{context}\n\n"
            f"Question:\n{question}\n\n"
            "Answer in a detailed paragraph:"
        )

        result = self.model(
            prompt,
            max_length=512,
            do_sample=False
        )

        answer = result[0]["generated_text"]

        retrieval_confidence = self._calculate_retrieval_confidence(context_chunks)
        confidence = min(0.6 + 0.4 * retrieval_confidence, 1.0)

        return answer, confidence

    def _generate_fallback(
        self, question: str, context_chunks: List[Dict]
    ) -> Tuple[str, float]:
        """
        Explanatory fallback answer generation (no LLM)
        """

        chunks = sorted(
            context_chunks, key=lambda x: x.get("score", 0), reverse=True
        )

        confidence = min(float(chunks[0].get("score", 0.0)), 1.0)

        sections = []
        for i, chunk in enumerate(chunks[:3], start=1):
            text = chunk["text"]
            if len(text) > 500:
                text = text[:500] + "..."
            sections.append(f"Source {i}:\n{text}")

        sources = ", ".join(
            sorted(set(c["metadata"]["filename"] for c in chunks[:3]))
        )

        answer = (
            f"Question:\n{question}\n\n"
            f"Answer:\n"
            f"Based on the uploaded documents ({sources}), the following information "
            f"is relevant:\n\n"
            f"{chr(10).join(sections)}\n\n"
            f"Explanation:\n"
            f"The answer was generated by combining multiple relevant document "
            f"chunks retrieved using semantic similarity search."
        )

        return answer, confidence

    def _calculate_retrieval_confidence(
        self, context_chunks: List[Dict]
    ) -> float:

        if not context_chunks:
            return 0.0

        scores = [c["score"] for c in context_chunks]
        return min(sum(scores) / len(scores), 1.0)
