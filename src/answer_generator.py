"""
Answer Generator Module
Generates answers using retrieved context and a language model
"""

from typing import List, Dict, Tuple
import logging

logger = logging.getLogger(__name__)


class AnswerGenerator:
    """
    Generates answers to questions using retrieved context
    """

    def __init__(self):
        self.model = None
        self._initialize_model()

    def _initialize_model(self):
        try:
            from transformers import pipeline

            logger.info("Initializing answer generation model...")
            self.model = pipeline(
                "question-answering",
                model="deepset/roberta-base-squad2",
                device=-1
            )
            logger.info("Answer generation model initialized")

        except Exception as e:
            logger.warning(f"Model not available, using fallback: {e}")
            self.model = None

    def generate_answer(
        self, question: str, context_chunks: List[Dict]
    ) -> Tuple[str, float]:

        if not context_chunks:
            return "I couldn't find relevant information to answer your question.", 0.0

        try:
            context = self._prepare_context(context_chunks)

            if self.model:
                return self._generate_with_model(question, context, context_chunks)

            return self._generate_fallback(question, context_chunks)

        except Exception as e:
            logger.error(f"Answer generation error: {e}")
            return self._generate_fallback(question, context_chunks)

    def _prepare_context(
        self, context_chunks: List[Dict], max_length: int = 1500
    ) -> str:

        parts = []
        total = 0

        for chunk in context_chunks:
            text = chunk["text"]
            if total + len(text) <= max_length:
                parts.append(text)
                total += len(text)
            else:
                parts.append(text[: max_length - total])
                break

        return " ".join(parts)

    def _generate_with_model(
        self, question: str, context: str, context_chunks: List[Dict]
    ) -> Tuple[str, float]:

        result = self.model(question=question, context=context)

        answer = result["answer"]
        model_conf = result["score"]

        retrieval_conf = self._calculate_retrieval_confidence(context_chunks)
        confidence = 0.6 * model_conf + 0.4 * retrieval_conf

        return self._enhance_answer(answer, model_conf), confidence

    def _generate_fallback(
        self, question: str, context_chunks: List[Dict]
    ) -> Tuple[str, float]:
        """
        Explanatory fallback answer generation (no LLM)
        """

        chunks = sorted(
            context_chunks, key=lambda x: x.get("score", 0), reverse=True
        )

        confidence = min(float(chunks[0].get("score", 0.0)), 1.0)

        sections = []
        for i, chunk in enumerate(chunks[:3], 1):
            text = chunk["text"][:500]
            sections.append(f"Source {i}:\n{text}")

        sources = ", ".join(
            sorted(set(c["metadata"]["filename"] for c in chunks[:3]))
        )

        answer = (
            f"Question:\n{question}\n\n"
            f"Answer:\n"
            f"Based on the uploaded documents ({sources}), the following "
            f"information is relevant:\n\n"
            f"{chr(10).join(sections)}\n\n"
            f"Explanation:\n"
            f"The answer was generated by combining multiple relevant document "
            f"chunks retrieved using semantic similarity search."
        )

        return answer, confidence

    def _enhance_answer(
        self, answer: str, base_confidence: float
    ) -> str:
        if base_confidence < 0.5:
            return f"Based on the available information: {answer}"
        return answer

    def _calculate_retrieval_confidence(
        self, context_chunks: List[Dict]
    ) -> float:

        scores = [c["score"] for c in context_chunks]
        return min(sum(scores) / len(scores), 1.0)
